{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import DataFrame\n",
    "import re\n",
    "import time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import boto3.session\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField, DateType\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from pyspark.sql.functions import col, count, udf, min, to_date, max, countDistinct, lit, when\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d935ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'test_ncp'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.ERROR)\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "logging.info('os.getenv: {}'.format(os.getenv))\n",
    "\n",
    "partition = sys.argv[1]\n",
    "execution_date = sys.argv[2]\n",
    "execution_date = datetime.fromisoformat(execution_date)\n",
    "date_partition = datetime.strftime(execution_date, 'date=%Y-%m-%d/hour=%H')\n",
    "\n",
    "logging.info(\"----------------------------------\")\n",
    "logging.info(f\"partition: {partition}\")\n",
    "logging.info(f\"execution_date: {execution_date}\")\n",
    "logging.info(f\"date_partition: {date_partition}\")\n",
    "logging.info(\"github MikhailYurkov\")\n",
    "\n",
    "dataset = 'messages-sms-current'\n",
    "data_path = 's3a://oksk/data/abonent_vector/taxonomy_first_layer/{}/*.parquet'.format(date_partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44245c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyDataFromS3(read_path, write_path, date1 = None, date2 = None, query, rpt):\n",
    "    if date1 != None:\n",
    "        start_date = datetime.strptime(date1, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(date2, \"%Y-%m-%d\")\n",
    "        for date in tqdm(pd.date_range(start=start_date,end=end_date, freq='m')):\n",
    "            df = spark.read.format(\"parquet\").load(pattern_path.format(date.strftime('%Y-%m-%d')))\n",
    "            df.createOrReplaceTempView(\"df\")\n",
    "            df2 = spark.sql(query)\n",
    "            df2 = df2.repartition(rpt)\n",
    "            df2.write.mode('append').parquet(write_path)\n",
    "    else:\n",
    "        df = spark.read.format(\"parquet\").load(pattern_path))\n",
    "        df.createOrReplaceTempView(\"df\")\n",
    "        df2 = spark.sql(query)\n",
    "        df2 = df2.repartition(rpt)\n",
    "        df2.write.mode('append').parquet(write_path)\n",
    "    \n",
    "    return spark.read.format(\"parquet\").load(write_path.format(date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ea1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataToQuery(pattern_path):\n",
    "    df_mas_tax = spark.read.format(\"parquet\").load(pattern_path)\n",
    "\n",
    "    df_mas_tax.createOrReplaceTempView(\"df\")\n",
    "    df_mas_tax = spark.sql('''  \n",
    "                    SELECT * \n",
    "                    from df\n",
    "                    ''')\n",
    "\n",
    "    ms = df_mas_tax.columns\n",
    "    ms.remove('phone')\n",
    "    ms_ch1 = ''\n",
    "    ms_ch1_sum = ''\n",
    "    ms_ch1_sum_2 = ''\n",
    "    ms_ch1_count = ''\n",
    "    for i in ms:\n",
    "        ms_ch1 += (',`' + i + '`')\n",
    "        ms_ch1_sum += (' + `' + i + '`')\n",
    "        ms_ch1_sum_2 += (',sum(`' + i + '`)')\n",
    "        ms_ch1_count += (' + count(case when `' + i + '` != 0 then 1 end)')\n",
    "    return ms_ch1, ms_ch1_sum, ms_ch1_sum_2, ms_ch1_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04649fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformAssembler(df):\n",
    "    df1p = df.toPandas()\n",
    "    k1p = df1p.columns\n",
    "    a = 0\n",
    "    k1 = []\n",
    "    b = 0\n",
    "    k2 = []\n",
    "    for i in range(0,df1p.shape[1]):\n",
    "        if df1p.mean()[k1p[i]] < 2*10**(-2):\n",
    "            k1.append(k1p[i])\n",
    "            a += 1\n",
    "        else:\n",
    "            k2.append(k1p[i])\n",
    "            b +=1\n",
    "    k2.pop(0)\n",
    "\n",
    "    assembler1 = VectorAssembler(\n",
    "        inputCols=k1,\n",
    "        outputCol=\"first_features\"\n",
    "    )\n",
    "    output1 = assembler1.transform(df1)\n",
    "\n",
    "\n",
    "\n",
    "    pca1 = PCA(k=5, inputCol=\"first_features\", outputCol=\"pcaFeatures_old\")\n",
    "    model1 = pca1.fit(output1)\n",
    "    result1 = model1.transform(output1).select('phone', \"pcaFeatures_old\")\n",
    "\n",
    "    assembler1 = VectorAssembler(\n",
    "        inputCols=k2,\n",
    "        outputCol=\"sec_features\"\n",
    "    )\n",
    "    output1 = assembler1.transform(df1)\n",
    "\n",
    "\n",
    "    pca1 = PCA(k=15, inputCol=\"sec_features\", outputCol=\"pcaFeatures_old2\")\n",
    "    model1 = pca1.fit(output1)\n",
    "    result12 = model1.transform(output1).select('phone', \"pcaFeatures_old2\")\n",
    "    result1 = result1.join(result12, on='phone', how='inner')\n",
    "\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/ncp_11_pca/result1_1'\n",
    "    result1.write.mode('overwrite').parquet(write_path)\n",
    "    k1r = []\n",
    "    for i in result1.columns:\n",
    "        k1r.append(i)\n",
    "    k1r.pop(0)\n",
    "\n",
    "    assembler11 = VectorAssembler(\n",
    "        inputCols=k1r,\n",
    "        outputCol=\"just_features\"\n",
    "    )\n",
    "    result1 = assembler11.transform(result1)\n",
    "\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/ncp_11_pca/result1'\n",
    "    result1.write.mode('overwrite').parquet(write_path)\n",
    "    return result1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09497a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName('from-s3-to-s3-{}-{}'.format(name, date_partition)).getOrCreate()\n",
    "\n",
    "    logging.info(\"Загрузка данных\")\n",
    "    pattern_path = 's3a://oksk/data/abonent_vector/taxonomy_first_layer/date={}'\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_old'\n",
    "\n",
    "    query = '''  \n",
    "                         SELECT * \n",
    "                         from df\n",
    "                         '''\n",
    "    df1 = copyDataFromS3(pattern_path, write_path, date1 = '2022-01-01' , date2 = '2022-07-01' ,query, 15)    \n",
    "    pattern_path = 's3a://oksk/data/abonent_vector/taxonomy_first_layer/date={}'\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_new'\n",
    "\n",
    "    query = '''  \n",
    "                         SELECT * \n",
    "                         from df\n",
    "                         '''\n",
    "    df2 = copyDataFromS3(pattern_path, write_path, date1 = '2023-01-01' , date2 = '2023-07-01' ,query = query, 15)\n",
    "\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_old'\n",
    "    ms_ch1, ms_ch1_sum, ms_ch1_sum_2, ms_ch1_count = getDataToQuery(pattern_path)\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_new'\n",
    "    ms_ch2, ms_ch2_sum, ms_ch2_sum_2, ms_ch2_count = getDataToQuery(pattern_path)\n",
    "\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_old'\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_old_gr_by'\n",
    "\n",
    "    query = f'''  \n",
    "                    select phone {ms_ch1_sum_2}\n",
    "                    from df\n",
    "                    group by phone\n",
    "                    '''\n",
    "    df1 = copyDataFromS3(pattern_path, write_path, query = query, rpt = 10)\n",
    "\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_new'\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_new_gr_by'\n",
    "\n",
    "    query = f'''  \n",
    "                    select phone {ms_ch2_sum_2}\n",
    "                    from df\n",
    "                    group by phone\n",
    "                    '''\n",
    "    df2 = copyDataFromS3(pattern_path, write_path, query = query, rpt = 10)\n",
    "\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_old_gr_by'\n",
    "    ms_ch1, ms_ch1_sum, ms_ch1_sum_2, ms_ch1_count = getDataToQuery(pattern_path)\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_new_gr_by'\n",
    "    ms_ch2, ms_ch2_sum, ms_ch2_sum_2, ms_ch2_count = getDataToQuery(pattern_path)\n",
    "\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_old_gr_by'\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/ncp_11_pca/all_old_gr_by_without_emissions'\n",
    "\n",
    "    query = f'''\n",
    "                    select *\n",
    "                    from df\n",
    "                    where phone in(\n",
    "                        select phone\n",
    "                        from df\n",
    "                        group by phone\n",
    "                        having 0 {ms_ch1_count} > 1\n",
    "                    )\n",
    "                    '''\n",
    "    df1 = copyDataFromS3(pattern_path, write_path, query = query, rpt = 10)\n",
    "\n",
    "    pattern_path = 's3a://oksk/masterdata/target/mike_test/NCP/numbers_change_people/all_new_gr_by'\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/ncp_11_pca/all_new_gr_by_without_emissions'\n",
    "\n",
    "    query = f'''\n",
    "                    select *\n",
    "                    from df\n",
    "                    where phone in(\n",
    "                        select phone\n",
    "                        from df\n",
    "                        group by phone\n",
    "                        having 0 {ms_ch2_count} > 1\n",
    "                    )\n",
    "                    '''\n",
    "    df2 = copyDataFromS3(pattern_path, write_path, query = query, rpt = 10)\n",
    "\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/ncp_11_pca/all_old_gr_by_without_emissions'\n",
    "    df1 = spark.read.format(\"parquet\").load(write_path)\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/ncp_11_pca/all_new_gr_by_without_emissions'\n",
    "    df2 = spark.read.format(\"parquet\").load(write_path)\n",
    "\n",
    "    result1 = transformAssembler(df1)\n",
    "    result2 = transformAssembler(df2)\n",
    "\n",
    "\n",
    "    df_p = result1.join(result2, on='phone', how='inner').select(['phone', \"just_features\", \"just_features2\"])\n",
    "\n",
    "    dot_udf = udf(lambda x, y: float(x.dot(y)), FloatType())\n",
    "    niz_udf = udf(lambda x, y: float(x.dot(x) ** 0.5 * y.dot(y) ** 0.5), FloatType())\n",
    "\n",
    "    df_p = df_p.withColumn('cos_taxon', dot_udf(col('just_features'), col('just_features2'))\n",
    "                        / niz_udf(col('just_features'), col('just_features2')))\n",
    "\n",
    "    write_path = 's3a://oksk/masterdata/target/mike_test/NCP/target/{}'.format(date_partition)\n",
    "    df_p.filter('cos_taxon > -0.001 and cos_taxon < 0.001').write.mode('overwrite').parquet(write_path)\n",
    "    \n",
    "    logging.info(\"Запись в хранилище\")\n",
    "    df1.write.mode('overwrite').parquet(write_path)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109510b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f108f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731e98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed7ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c181a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb0520c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe660a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
